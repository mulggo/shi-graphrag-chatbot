#!/usr/bin/env python3
"""
PWRU19RDNE KB ì½˜í…ì¸  íŒ¨í„´ ë¶„ì„
ë‹¤ì–‘í•œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰í•˜ì—¬ ì½˜í…ì¸  íƒ€ì… ë¶„ë¥˜
"""

import boto3
import json

def analyze_pwru_content_patterns():
    """PWRU19RDNE KB ì½˜í…ì¸  íŒ¨í„´ ë¶„ì„"""
    
    bedrock_agent = boto3.client('bedrock-agent-runtime', region_name='us-west-2')
    
    # ë‹¤ì–‘í•œ ì¿¼ë¦¬ë¡œ í…ŒìŠ¤íŠ¸
    queries = [
        "ì†Œí™”ê¸°",
        "í™”ì¬ ê°ì§€",
        "ìŠ¤í”„ë§í´ëŸ¬",
        "ì•ˆì „ ê·œì •",
        "SOLAS",
        "ì„ ë°• êµ¬ì¡°",
        "ë°©í™”ë²½",
        "ë¹„ìƒ íƒˆì¶œ"
    ]
    
    all_results = []
    content_types = {
        'ai_conversation': 0,  # "I understand..." íƒ€ì…
        'structured_summary': 0,  # "# Fire Safety..." íƒ€ì…
        'other': 0
    }
    
    for query in queries:
        print(f"\nğŸ” ì¿¼ë¦¬: '{query}'")
        
        try:
            response = bedrock_agent.retrieve(
                knowledgeBaseId='PWRU19RDNE',
                retrievalQuery={'text': query},
                retrievalConfiguration={
                    'vectorSearchConfiguration': {
                        'numberOfResults': 5
                    }
                }
            )
            
            for i, result in enumerate(response.get('retrievalResults', [])):
                content = result.get('content', {})
                text = content.get('text', '')
                page_num = result.get('metadata', {}).get('x-amz-bedrock-kb-document-page-number', 'N/A')
                
                # ì½˜í…ì¸  íƒ€ì… ë¶„ë¥˜
                content_type = classify_content_type(text)
                content_types[content_type] += 1
                
                print(f"  ê²°ê³¼ {i+1} (í˜ì´ì§€ {page_num}): {content_type}")
                print(f"    ê¸¸ì´: {len(text)}ì")
                print(f"    ì‹œì‘: {text[:100]}...")
                
                all_results.append({
                    'query': query,
                    'page_number': page_num,
                    'content_type': content_type,
                    'length': len(text),
                    'preview': text[:200]
                })
                
        except Exception as e:
            print(f"  âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
    
    # í†µê³„ ì¶œë ¥
    print(f"\nğŸ“Š ì½˜í…ì¸  íƒ€ì… í†µê³„:")
    total = sum(content_types.values())
    for content_type, count in content_types.items():
        percentage = (count / total * 100) if total > 0 else 0
        print(f"  {content_type}: {count}ê°œ ({percentage:.1f}%)")
    
    # í˜ì´ì§€ë³„ ë¶„ì„
    analyze_by_page(all_results)
    
    return all_results

def classify_content_type(text):
    """ì½˜í…ì¸  íƒ€ì… ë¶„ë¥˜"""
    
    if not text:
        return 'other'
    
    # AI ëŒ€í™”í˜• ì‘ë‹µ
    if any(phrase in text for phrase in [
        "I understand. I will not reproduce",
        "I'll summarize and discuss",
        "Here is a summary of the key points"
    ]):
        return 'ai_conversation'
    
    # êµ¬ì¡°í™”ëœ ìš”ì•½
    if text.startswith('#') or any(phrase in text for phrase in [
        "This document covers",
        "Key points include:",
        "## Water Supply Systems",
        "# Fire Safety"
    ]):
        return 'structured_summary'
    
    return 'other'

def analyze_by_page(results):
    """í˜ì´ì§€ë³„ ì½˜í…ì¸  íƒ€ì… ë¶„ì„"""
    
    page_analysis = {}
    
    for result in results:
        page_num = result['page_number']
        content_type = result['content_type']
        
        if page_num not in page_analysis:
            page_analysis[page_num] = {}
        
        if content_type not in page_analysis[page_num]:
            page_analysis[page_num][content_type] = 0
        
        page_analysis[page_num][content_type] += 1
    
    print(f"\nğŸ“„ í˜ì´ì§€ë³„ ì½˜í…ì¸  íƒ€ì…:")
    for page_num in sorted(page_analysis.keys(), key=lambda x: float(x) if x != 'N/A' else 999):
        types = page_analysis[page_num]
        print(f"  í˜ì´ì§€ {page_num}: {types}")

if __name__ == "__main__":
    print("ğŸ” PWRU19RDNE KB ì½˜í…ì¸  íŒ¨í„´ ë¶„ì„")
    analyze_pwru_content_patterns()