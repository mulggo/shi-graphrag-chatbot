"""
GraphRAG Performance Testing - í‰ê·  ì‘ë‹µ ì‹œê°„ 30ì´ˆ ì´ë‚´ ê²€ì¦

ì´ ìŠ¤í¬ë¦½íŠ¸ëŠ” GraphRAG ë©€í‹° ì—ì´ì „íŠ¸ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•˜ê³ 
í‰ê·  ì‘ë‹µ ì‹œê°„ì´ 30ì´ˆ ì´ë‚´ì¸ì§€ ê²€ì¦í•©ë‹ˆë‹¤.

Requirements: 9.7 (Performance testing - ì‘ë‹µ ì‹œê°„ 30ì´ˆ ì´ë‚´)

í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤:
1. ë‹¨ìˆœ ì‚¬ì‹¤ í™•ì¸ ì§ˆë¬¸ (ì˜ˆìƒ: 15-20ì´ˆ)
2. ë‹¤ì¤‘ ë¬¸ì„œ ì¶”ë¡  ì§ˆë¬¸ (ì˜ˆìƒ: 20-25ì´ˆ)
3. ë³µì¡í•œ ë¹„êµ ë¶„ì„ ì§ˆë¬¸ (ì˜ˆìƒ: 25-30ì´ˆ)
4. 11ê°œ ë¬¸ì„œ ì»¤ë²„ë¦¬ì§€ í…ŒìŠ¤íŠ¸ (ë‹¤ì–‘í•œ ë¬¸ì„œ ìœ í˜•)

ì„±ëŠ¥ ëª©í‘œ:
- í‰ê·  ì‘ë‹µ ì‹œê°„: < 30ì´ˆ
- 95 percentile: < 35ì´ˆ
- ìµœëŒ€ ì‘ë‹µ ì‹œê°„: < 45ì´ˆ
"""
import time
import statistics
import json
from typing import List, Dict, Tuple
from datetime import datetime
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PerformanceTestResult:
    """ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼"""
    
    def __init__(self, query: str, duration: float, success: bool, metadata: Dict = None):
        self.query = query
        self.duration = duration
        self.success = success
        self.metadata = metadata or {}
        self.timestamp = datetime.now()
    
    def to_dict(self) -> Dict:
        return {
            'query': self.query,
            'duration': self.duration,
            'success': self.success,
            'metadata': self.metadata,
            'timestamp': self.timestamp.isoformat()
        }


class PerformanceTester:
    """GraphRAG ì„±ëŠ¥ í…ŒìŠ¤í„°"""
    
    def __init__(self, agent):
        """
        ì„±ëŠ¥ í…ŒìŠ¤í„° ì´ˆê¸°í™”
        
        Args:
            agent: GraphRAG Agent ì¸ìŠ¤í„´ìŠ¤
        """
        self.agent = agent
        self.results: List[PerformanceTestResult] = []
    
    def run_single_test(self, query: str, session_id: str = "perf-test") -> PerformanceTestResult:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        
        Args:
            query: í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
            session_id: ì„¸ì…˜ ID
            
        Returns:
            PerformanceTestResult: í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        logger.info(f"í…ŒìŠ¤íŠ¸ ì‹œì‘: '{query[:50]}...'")
        
        start_time = time.time()
        
        try:
            response = self.agent.process_message(query, session_id)
            duration = time.time() - start_time
            
            success = response.get('success', False)
            metadata = response.get('metadata', {})
            
            result = PerformanceTestResult(
                query=query,
                duration=duration,
                success=success,
                metadata=metadata
            )
            
            logger.info(f"í…ŒìŠ¤íŠ¸ ì™„ë£Œ: duration={duration:.2f}s, success={success}")
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {str(e)}")
            
            return PerformanceTestResult(
                query=query,
                duration=duration,
                success=False,
                metadata={'error': str(e)}
            )
    
    def run_test_suite(self, test_queries: List[Tuple[str, str]]) -> List[PerformanceTestResult]:
        """
        í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹¤í–‰
        
        Args:
            test_queries: (query, category) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            List[PerformanceTestResult]: í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        logger.info(f"í…ŒìŠ¤íŠ¸ ìŠ¤ìœ„íŠ¸ ì‹œì‘: {len(test_queries)}ê°œ ì¿¼ë¦¬")
        
        results = []
        
        for i, (query, category) in enumerate(test_queries, 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"í…ŒìŠ¤íŠ¸ {i}/{len(test_queries)}: {category}")
            logger.info(f"{'='*80}")
            
            result = self.run_single_test(query, f"perf-test-{i}")
            result.metadata['category'] = category
            results.append(result)
            
            # ê²°ê³¼ ì €ì¥
            self.results.append(result)
            
            # í…ŒìŠ¤íŠ¸ ê°„ ê°„ê²© (Lambda ì½œë“œ ìŠ¤íƒ€íŠ¸ ë°©ì§€)
            if i < len(test_queries):
                time.sleep(2)
        
        return results
    
    def analyze_results(self, results: List[PerformanceTestResult]) -> Dict:
        """
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„
        
        Args:
            results: í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            Dict: ë¶„ì„ ê²°ê³¼
        """
        if not results:
            return {
                'error': 'No test results available'
            }
        
        # ì„±ê³µí•œ í…ŒìŠ¤íŠ¸ë§Œ ë¶„ì„
        successful_results = [r for r in results if r.success]
        
        if not successful_results:
            return {
                'error': 'No successful tests',
                'total_tests': len(results),
                'failed_tests': len(results)
            }
        
        durations = [r.duration for r in successful_results]
        
        # ê¸°ë³¸ í†µê³„
        avg_duration = statistics.mean(durations)
        median_duration = statistics.median(durations)
        min_duration = min(durations)
        max_duration = max(durations)
        
        # í‘œì¤€ í¸ì°¨
        stdev_duration = statistics.stdev(durations) if len(durations) > 1 else 0
        
        # Percentiles
        sorted_durations = sorted(durations)
        p95_index = int(len(sorted_durations) * 0.95)
        p99_index = int(len(sorted_durations) * 0.99)
        p95_duration = sorted_durations[p95_index] if p95_index < len(sorted_durations) else max_duration
        p99_duration = sorted_durations[p99_index] if p99_index < len(sorted_durations) else max_duration
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        category_stats = {}
        for result in successful_results:
            category = result.metadata.get('category', 'unknown')
            if category not in category_stats:
                category_stats[category] = []
            category_stats[category].append(result.duration)
        
        category_analysis = {}
        for category, durations in category_stats.items():
            category_analysis[category] = {
                'count': len(durations),
                'avg_duration': statistics.mean(durations),
                'min_duration': min(durations),
                'max_duration': max(durations)
            }
        
        # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
        meets_avg_target = avg_duration < 30.0
        meets_p95_target = p95_duration < 35.0
        meets_max_target = max_duration < 45.0
        
        analysis = {
            'total_tests': len(results),
            'successful_tests': len(successful_results),
            'failed_tests': len(results) - len(successful_results),
            'success_rate': len(successful_results) / len(results) * 100,
            'duration_stats': {
                'average': avg_duration,
                'median': median_duration,
                'min': min_duration,
                'max': max_duration,
                'stdev': stdev_duration,
                'p95': p95_duration,
                'p99': p99_duration
            },
            'performance_targets': {
                'avg_under_30s': {
                    'target': 30.0,
                    'actual': avg_duration,
                    'met': meets_avg_target,
                    'margin': 30.0 - avg_duration
                },
                'p95_under_35s': {
                    'target': 35.0,
                    'actual': p95_duration,
                    'met': meets_p95_target,
                    'margin': 35.0 - p95_duration
                },
                'max_under_45s': {
                    'target': 45.0,
                    'actual': max_duration,
                    'met': meets_max_target,
                    'margin': 45.0 - max_duration
                }
            },
            'category_analysis': category_analysis,
            'overall_pass': meets_avg_target and meets_p95_target and meets_max_target
        }
        
        return analysis
    
    def print_results(self, analysis: Dict):
        """
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶œë ¥
        
        Args:
            analysis: ë¶„ì„ ê²°ê³¼
        """
        print("\n" + "="*80)
        print("GraphRAG ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼")
        print("="*80)
        
        if 'error' in analysis:
            print(f"\nâŒ ì—ëŸ¬: {analysis['error']}")
            return
        
        # ê¸°ë³¸ í†µê³„
        print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ í†µê³„:")
        print(f"  - ì´ í…ŒìŠ¤íŠ¸: {analysis['total_tests']}")
        print(f"  - ì„±ê³µ: {analysis['successful_tests']}")
        print(f"  - ì‹¤íŒ¨: {analysis['failed_tests']}")
        print(f"  - ì„±ê³µë¥ : {analysis['success_rate']:.1f}%")
        
        # ì‘ë‹µ ì‹œê°„ í†µê³„
        stats = analysis['duration_stats']
        print(f"\nâ±ï¸  ì‘ë‹µ ì‹œê°„ í†µê³„:")
        print(f"  - í‰ê· : {stats['average']:.2f}ì´ˆ")
        print(f"  - ì¤‘ì•™ê°’: {stats['median']:.2f}ì´ˆ")
        print(f"  - ìµœì†Œ: {stats['min']:.2f}ì´ˆ")
        print(f"  - ìµœëŒ€: {stats['max']:.2f}ì´ˆ")
        print(f"  - í‘œì¤€í¸ì°¨: {stats['stdev']:.2f}ì´ˆ")
        print(f"  - 95 percentile: {stats['p95']:.2f}ì´ˆ")
        print(f"  - 99 percentile: {stats['p99']:.2f}ì´ˆ")
        
        # ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„± ì—¬ë¶€
        targets = analysis['performance_targets']
        print(f"\nğŸ¯ ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±:")
        
        for target_name, target_data in targets.items():
            met = target_data['met']
            icon = "âœ…" if met else "âŒ"
            print(f"  {icon} {target_name}:")
            print(f"     ëª©í‘œ: {target_data['target']:.1f}ì´ˆ")
            print(f"     ì‹¤ì œ: {target_data['actual']:.2f}ì´ˆ")
            print(f"     ì—¬ìœ : {target_data['margin']:.2f}ì´ˆ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
        if analysis['category_analysis']:
            print(f"\nğŸ“ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„:")
            for category, cat_stats in analysis['category_analysis'].items():
                print(f"  - {category}:")
                print(f"    í…ŒìŠ¤íŠ¸ ìˆ˜: {cat_stats['count']}")
                print(f"    í‰ê· : {cat_stats['avg_duration']:.2f}ì´ˆ")
                print(f"    ë²”ìœ„: {cat_stats['min_duration']:.2f}ì´ˆ ~ {cat_stats['max_duration']:.2f}ì´ˆ")
        
        # ì „ì²´ ê²°ê³¼
        overall_pass = analysis['overall_pass']
        print(f"\n{'='*80}")
        if overall_pass:
            print("âœ… ì „ì²´ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼!")
            print("   í‰ê·  ì‘ë‹µ ì‹œê°„ì´ 30ì´ˆ ì´ë‚´ì´ë©°, ëª¨ë“  ì„±ëŠ¥ ëª©í‘œë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("âŒ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            print("   ì¼ë¶€ ì„±ëŠ¥ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìµœì í™”ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("="*80 + "\n")
    
    def save_results(self, filename: str = "performance_test_results.json"):
        """
        í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            filename: ì €ì¥í•  íŒŒì¼ëª…
        """
        results_data = {
            'timestamp': datetime.now().isoformat(),
            'results': [r.to_dict() for r in self.results],
            'analysis': self.analyze_results(self.results)
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {filename}")


def get_test_queries() -> List[Tuple[str, str]]:
    """
    ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ëª©ë¡
    
    11ê°œ ë¬¸ì„œë¥¼ ê· í˜•ìˆê²Œ ì»¤ë²„í•˜ëŠ” ë‹¤ì–‘í•œ ì§ˆë¬¸ ìœ í˜•
    
    Returns:
        List[Tuple[str, str]]: (query, category) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
    """
    return [
        # 1. ë‹¨ìˆœ ì‚¬ì‹¤ í™•ì¸ (FSS Code)
        ("ê³ ì •ì‹ CO2 ì†Œí™” ì‹œìŠ¤í…œì˜ ìµœì†Œ ìš©ëŸ‰ì€?", "simple_factual"),
        
        # 2. ë‹¨ìˆœ ì‚¬ì‹¤ í™•ì¸ (SOLAS)
        ("í™”ì¬ ê°ì§€ ì‹œìŠ¤í…œì˜ ì„¤ì¹˜ ìš”êµ¬ì‚¬í•­ì€?", "simple_factual"),
        
        # 3. ì„¤ê³„ ì§€ì¹¨ (Design guidance)
        ("ë°°ê´€ ì§€ì§€ëŒ€ ì„¤ê³„ ì‹œ ê³ ë ¤ì‚¬í•­ì€?", "design_guidance"),
        
        # 4. ì‹¤ë¬´ ë¬¸ì„œ (Piping practice)
        ("ë°°ê´€ ê´€í†µë¶€ ì‹œê³µ ì‹œ ì£¼ì˜ì‚¬í•­ì€?", "practice"),
        
        # 5. DNV ê·œì •
        ("DNV ì„ ê¸‰ ê·œì¹™ì—ì„œ ë‹¨ì—´ì¬ ìš”êµ¬ì‚¬í•­ì€?", "dnv_rules"),
        
        # 6. IGC Code
        ("IGC Codeì— ë”°ë¥¸ ê°€ìŠ¤ ìš´ë°˜ì„ ì˜ ì†Œí™” ì‹œìŠ¤í…œì€?", "igc_code"),
        
        # 7. ë‹¤ì¤‘ ë¬¸ì„œ ì¶”ë¡  (SOLAS + Design guidance)
        ("ì„ ì²´ ê´€í†µë¶€ì˜ ë‹¨ì—´ì¬ ì„¤ê³„ ê¸°ì¤€ê³¼ ì‹œê³µ ë°©ë²•ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”", "multi_doc"),
        
        # 8. ë¹„êµ ë¶„ì„ (Design guidance vs Practice)
        ("ë°°ê´€ ì§€ì§€ëŒ€ ì„¤ê³„ ê°€ì´ë“œì™€ ì‹¤ì œ ì‹œê³µ ë°©ë²•ì˜ ì°¨ì´ì ì€?", "comparative"),
        
        # 9. ë³µì¡í•œ ë‹¤ì¤‘ ë¬¸ì„œ (DNV + Design guidance + Practice)
        ("DNV ê·œì •ì— ë”°ë¥¸ ë°°ê´€ ì§€ì§€ëŒ€ ì„¤ê³„ ê¸°ì¤€ê³¼ ì‹¤ì œ ì‹œê³µ ì‹œ ì£¼ì˜ì‚¬í•­ì„ ë¹„êµí•´ì£¼ì„¸ìš”", "complex_multi_doc"),
        
        # 10. ì ˆì°¨ ë¬¸ì„œ (Spoolcutting)
        ("ìŠ¤í’€ ì ˆë‹¨ ì‘ì—… ì‹œ ì•ˆì „ ì ˆì°¨ëŠ”?", "procedure"),
        
        # 11. ì¢…í•© ì§ˆë¬¸ (ì—¬ëŸ¬ ë¬¸ì„œ í†µí•©)
        ("ì„ ë°• ê¸°ê´€ì‹¤ì˜ í™”ì¬ ì•ˆì „ ì‹œìŠ¤í…œ ì „ì²´ êµ¬ì„±ê³¼ ê° ê·œì •ë³„ ìš”êµ¬ì‚¬í•­ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”", "comprehensive"),
    ]


def run_performance_test(agent, save_results: bool = True) -> Dict:
    """
    ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    Args:
        agent: GraphRAG Agent ì¸ìŠ¤í„´ìŠ¤
        save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
        
    Returns:
        Dict: ë¶„ì„ ê²°ê³¼
    """
    tester = PerformanceTester(agent)
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ê°€ì ¸ì˜¤ê¸°
    test_queries = get_test_queries()
    
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    logger.info(f"ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘: {len(test_queries)}ê°œ ì¿¼ë¦¬")
    results = tester.run_test_suite(test_queries)
    
    # ê²°ê³¼ ë¶„ì„
    analysis = tester.analyze_results(results)
    
    # ê²°ê³¼ ì¶œë ¥
    tester.print_results(analysis)
    
    # ê²°ê³¼ ì €ì¥
    if save_results:
        tester.save_results()
    
    return analysis


if __name__ == "__main__":
    """
    ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    
    ì£¼ì˜: ì´ í…ŒìŠ¤íŠ¸ëŠ” Lambda í•¨ìˆ˜ê°€ ë°°í¬ëœ í›„ì—ë§Œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    """
    print("\n" + "="*80)
    print("GraphRAG ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("="*80)
    print("\nâš ï¸  ì£¼ì˜: ì´ í…ŒìŠ¤íŠ¸ëŠ” Lambda í•¨ìˆ˜ê°€ ë°°í¬ëœ í›„ì—ë§Œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    print("   Lambda í•¨ìˆ˜ ë°°í¬ ë°©ë²•:")
    print("   1. cd lambda_package/graphrag_tools/")
    print("   2. ./deploy.sh")
    print("   3. config/agents.yamlì— Lambda ARN ì„¤ì •")
    print("\n" + "="*80 + "\n")
    
    # Agent ì´ˆê¸°í™” (ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” configì—ì„œ ë¡œë“œ)
    try:
        from agents.graphrag_agent.agent import Agent
        from core.agent_manager import AgentConfig
        import os
        
        # í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        required_env_vars = [
            'BEDROCK_KB_ID',
            'LAMBDA_CLASSIFY_QUERY_ARN',
            'LAMBDA_EXTRACT_ENTITIES_ARN',
            'LAMBDA_KB_RETRIEVE_ARN'
        ]
        
        missing_vars = [var for var in required_env_vars if not os.getenv(var)]
        
        if missing_vars:
            print(f"âŒ í•„ìˆ˜ í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤:")
            for var in missing_vars:
                print(f"   - {var}")
            print("\n.env íŒŒì¼ì„ í™•ì¸í•˜ê³  í•„ìš”í•œ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            exit(1)
        
        # Agent ì„¤ì •
        config = AgentConfig(
            name='graphrag',
            display_name='GraphRAG ê²€ìƒ‰',
            knowledge_base_id=os.getenv('BEDROCK_KB_ID'),
            lambda_function_names={
                'classify_query': os.getenv('LAMBDA_CLASSIFY_QUERY_ARN'),
                'extract_entities': os.getenv('LAMBDA_EXTRACT_ENTITIES_ARN'),
                'kb_retrieve': os.getenv('LAMBDA_KB_RETRIEVE_ARN')
            },
            reranker_model_arn=os.getenv('RERANKER_MODEL_ARN'),
            bedrock_model_id=os.getenv('BEDROCK_MODEL_ID', 'anthropic.claude-3-5-sonnet-20240620-v1:0'),
            metrics_enabled=True
        )
        
        # Agent ì´ˆê¸°í™”
        agent = Agent(config)
        
        # ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        analysis = run_performance_test(agent, save_results=True)
        
        # ì¢…ë£Œ ì½”ë“œ ì„¤ì • (CI/CD í†µí•©ìš©)
        exit_code = 0 if analysis.get('overall_pass', False) else 1
        exit(exit_code)
        
    except ImportError as e:
        print(f"âŒ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨: {str(e)}")
        print("   agents/graphrag_agent/ ë””ë ‰í† ë¦¬ê°€ ì˜¬ë°”ë¥´ê²Œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit(1)
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        import traceback
        traceback.print_exc()
        exit(1)
